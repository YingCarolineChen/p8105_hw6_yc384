---
title: "P8105_hw6_yc384"
author: 'Ying Chen (UNI: yc384)'
date: "11/5/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
library(readr)
library(ggridges)
library(purrr)
library(httr)
library(mgcv)
library(modelr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	message = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
  
set.seed(10)
```


## Problem 1: Understand the effects of several variables on a child’s birthweight.

#### Step 1: Load data

```{r, message = FALSE}
# read in data
birthweight = 
  read_csv("./data/birthweight.csv", col_names = TRUE)%>% 
  janitor::clean_names() 

head(birthweight, 3)
tail(birthweight, 3)
```

#### Step 2: EDA

```{r}
str(birthweight)
summary(birthweight)
skimr::skim(birthweight)
```

**Note:** From the above summary, we can see that this data contains 4342 observatons and 20 variables. At first glance, there are no missing data. However, looking at the other statistics, we can see that variables pnumlbw and pnumsga show 0 for all the statistics, which indicate these two variables maynot have valid information and we should exclude them from future analysis. 

```{r}
summary(birthweight$pnumlbw)
summary(birthweight$pnumsga)
```

#### Step 3: Tidy data

We then start tidying the data by converting numeirical variables to factor varaibles for babysex, father's race, mother's race and malformation history.


```{r}
birthweight_tidy = 
  birthweight %>% 
  distinct(instacart) %>% 
  mutate(
    babysex = factor(babysex, levels = c(1, 2), 
                     labels = c("male", "female")),
    frace = factor(frace, levels = c(1, 2, 3, 4, 8, 9), 
                   labels = c("white", "black", "asian", "puerto rican", "other", "unknown")),
    mrace = factor(mrace, levels = c(1, 2, 3, 4, 8, 9), 
                   labels = c("white", "black", "asian", "puerto rican", "other", "unknown")),
    malform = factor(malform, levels = c(0, 1), 
                     labels = c("absent", "present"))                         
  )

head(birthweight_tidy, 3)
```

#### Step 4: Fit a linear regression based on literature review for low birthweight. 

My model will use birth weight (bwt) as the outcome variable and the following variables as main effects: babysex, gestational age in weeks, baby’s length at birth and average number of cigarettes smoked per day during pregnancy. I also included a two-way interaction term of blength*bhead because they are correlated in general. 

```{r}
bwt_lm = 
    birthweight_tidy %>% 
    lm(bwt ~ babysex + gaweeks + blength + bhead + smoken + blength*bhead, data = .) 

summary(bwt_lm)
```

**Note:** From the model results, we can see that the two-way interaction term of blength*bhead is highly significant. All other main effects (except the ones in the interaction term) are also significant predictors. 

#### Step 5: Plot model residuals against fitted values use add_predictions and add_residuals

```{r, message = FALSE}
bwt_1 = modelr::add_residuals(birthweight_tidy, bwt_lm)
bwt_plot = modelr::add_predictions(bwt_1, bwt_lm)
bwt_plot %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  geom_smooth(se = F,size = 0.5) + 
  labs(title = " Residual vs Fitted Values") +
  theme(plot.title = element_text(hjust =.5)) +
  geom_line(aes(y = 0), color = "red", linetype = "dashed")
```

**Note:** From the above plot, we can see that my model fits the data very well. The residuals are around line 0 without any pattern. There are a couple of points with large residuals that need to be investigated to see if there are any data entry errors or any special cases. 

#### Step 6: Compare my model to two other models

**Main effect model** using length at birth and gestational age as predictors

```{r}
bwt_main = 
        birthweight_tidy %>% 
        lm(bwt ~ gaweeks + blength, data = .) 

summary(bwt_main)
```

**Three-way interaction model** using head circumference, length, sex, and all three-way interactions Make this comparison in terms of the cross-validated prediction error;

```{r}
bwt_interaction = 
        birthweight_tidy %>% 
        lm(bwt ~ babysex*blength*bhead, data = .) 

summary(bwt_main)
```

**Make comparison** in terms of the cross-validated prediction error using crossv_mc and functions in purrr.

```{r}
cv_df = 
  crossv_mc(birthweight_tidy, 100) 

cv_df =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

```{r}
cv_df = 
  cv_df %>% 
  mutate(
  my_mod=map(train, ~lm(bwt ~ babysex + gaweeks + blength + bhead + smoken + blength*bhead , data = .) ),
  main_mod=map(train, ~lm(bwt ~ gaweeks + blength, data = .)),
  interaction_mod=map(train, ~lm(bwt ~ babysex*blength*bhead, data = .)))%>% 
  mutate(
  rmse_my = map2_dbl(my_mod, test, ~rmse(model = .x, data = .y)),
  rmse_main = map2_dbl(main_mod, test, ~rmse(model = .x, data = .y)),
  rmse_interaction = map2_dbl(interaction_mod, test, ~rmse(model = .x, data = .y)))
```

```{r}
  cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(everything(),
               names_to = "model",
               values_to = "rmse",
               names_prefix = "rmse_"
  ) %>% 
  mutate(model = factor(model)) %>% 
  ggplot(aes(x = model, y = rmse,fill = model)) +
  geom_violin(alpha = 0.4) +
  geom_boxplot(width = 0.1, lwd = 0.7) +
  labs( title = "Model comparison by RMSE")
```

**Note:** From above plots, we can see that my model did the best with lowest RMSEs. The model will all three-way interaction terms had slighted higher RMSEs than my model. The model with only two main effects did the worst. 


## Problem 2: Use the 2017 Central Park weather data. We’ll focus on a simple linear regression with tmax as the response and tmin as the predictor, and are interested in the distribution of two quantities estimated from these data: r̂2, log(β̂0∗β̂1)

#### Step 1: Download data.

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

head(weather_df, 3)
```

#### Step 2: Bootstraping 5000 samples using broom::glance() for extracting r̂2 from a fitted regression, and broom::tidy() in computing log(β̂0∗β̂1)

```{r}
bootstrap_sample = 
  weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~ lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy),
    variables = map(models, broom::glance)
    ) %>% 
  select(-strap, -models) %>% 
  unnest(results, variables)

head(bootstrap_sample, 5)
```

#### Step 3: Plot the distribution of the estimates, and describe these in words. 

```{r}
plot_r2 = 
bootstrap_sample %>% 
  filter(term == "tmin") %>% 
  select(r.squared, adj.r.squared) %>%
  ggplot() + 
  geom_histogram(aes(x = r.squared, y =..density..), fill="grey") +
  geom_density(aes(x = r.squared, y=..density..)) +
  labs(
    title = "Distribution of Estimated RSquare",
    x = "Estimates of RSquare",
    y = "Density",
    caption = "Data: 2017 Central Park weather data ") +
  geom_vline(aes(xintercept = mean(r.squared)), color="red", linetype="dotted") +
  theme(plot.title = element_text(hjust = 0.5))

plot_r2
```

**Note:** From above plot, we can see that the distribution of R square is close to a normal distribution with mean a little over 0.91


#### Step 4: Using the 5000 bootstrap estimates, identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r̂2 an dlog(β̂0∗β̂1̂1)

* First, we compute the 95% Confidenc Interval (CI) for r̂2 and we then add the lower and upper bounds to the previous plot. 

```{r}
CI95_r2 = 
  bootstrap_sample %>% 
  filter(term == "tmin") %>% 
  pull(r.squared) %>% 
  quantile(c(0.025, 0.975))

CI95_r2

plot_r2 + 
geom_vline(aes(xintercept = 0.8938942), color="red", linetype="dotted") +
geom_vline(aes(xintercept = 0.9278097), color="red", linetype="dotted") +
labs(
    title = "Distribution of Estimated RSquare",
    x = "Estimates of RSquare",
    y = "Density"
    )
```

**Note:** From above plot, we can see that the distribution of R square is close to a bell shape, which indicates it follows a normal distribution.  The mean is a little over 0.91 and a 95% CI of (0.8938410, 0.9275271 ). This indicates that our model fits the data pretty well. 

* We then do a log transformation of the results and compute the 95% CI for log(β̂0∗β̂1) and we add the lower and upper bounds to the previous plot. 

```{r}
log_bs_sample =
  bootstrap_sample %>% 
  select(.id, term, estimate) %>% 
  pivot_wider(
    id = .id,
    values_from = estimate,
    names_from = term
  ) %>% 
  janitor::clean_names() %>% 
  mutate(log_b01 = log(intercept*tmin)) 
```

```{r}
CI95_logb01 = 
  log_bs_sample %>% 
  pull(log_b01) %>% 
  quantile(c(0.025, 0.975))

CI95_logb01
```


```{r, message  = FALSE}
log_bs_sample %>% 
  ggplot() + geom_histogram(aes(x = log_b01, y =..density..), fill = "grey") +
  labs(
    title = "Distribution Plot of the Estimate of log(β̂0 ∗ β̂1)",
    x = "Estimate of log(β̂0 ∗ β̂1)",
    y = "Density",
    caption = "Data: 2017 Central Park weather data") +
  geom_density(aes(x = log_b01, y=..density..)) +
  geom_vline(aes(xintercept = mean(log_b01)), colour="red", linetype="dotted") +
  geom_vline(aes(xintercept = 1.965296), colour="red", linetype="dotted") +
  geom_vline(aes(xintercept = 2.057525), colour="red", linetype="dotted") 
```

**Note:** The 95% CI for log(β̂0∗β̂1) is (1.964572, 2.059126). From the above plot, we can see that log(β̂0∗β̂1) follows a normal distributioasan as well. 

















